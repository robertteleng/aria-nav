# Aria Navigation System

> **Assistive navigation system for visually impaired users using Meta Aria glasses**  
> Combines computer vision, spatial analysis, and prioritized audio feedback in real-time.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status: Active Development](https://img.shields.io/badge/status-active-success.svg)]()

---

## âš¡ Quick Start

```bash
# Clone and install
git clone https://github.com/<your-user>/aria-nav.git
cd aria-nav
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Run with Aria hardware
python src/main.py

# Test without hardware
python src/main.py debug
```

**Controls:** `q` = quit | `t` = test audio | `Ctrl+C` = emergency stop

---

## ğŸ¯ What It Does

**Real-time navigation assistance using:**
- ğŸ¥ **RGB Camera** - Object detection (YOLO) + depth estimation (Depth-Anything v2) + fisheye rectification
- ğŸ‘€ **Peripheral Vision** - SLAM cameras for lateral obstacle detection (rectified)
- ğŸ§­ **IMU Sensors** - Motion state tracking (stationary/walking)
- ğŸ”Š **Spatial Audio** - Prioritized voice commands + beep alerts
- ğŸ“Š **Live Dashboards** - OpenCV, Rerun, or Web visualization
- ğŸ¯ **Image Rectification** - SDK-based undistortion for all fisheye cameras

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Meta Aria   â”‚â”€â”€â”€â–¶â”‚  Observer    â”‚â”€â”€â”€â–¶â”‚ Pipeline        â”‚
â”‚ (RGB+SLAM+  â”‚    â”‚ (SDK Bridge) â”‚    â”‚ (Vision + AI)   â”‚
â”‚  IMU)       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio       â”‚â—€â”€â”€â”€â”‚ Navigation   â”‚â—€â”€â”€â”€â”‚ Decision        â”‚
â”‚ System      â”‚    â”‚ Audio Router â”‚    â”‚ Engine          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**
- **Observer** - Hardware interface (cameras + IMU)
- **Pipeline** - Enhancement â†’ Depth â†’ Detection
- **Decision Engine** - Spatial reasoning and prioritization
- **Audio Router** - Cooldown management and queue coordination
- **Presentation** - Multi-dashboard rendering

---

## ğŸ“‹ Requirements

### Hardware
- Meta Aria glasses with `profile28` enabled
- Mac (Apple Silicon recommended) or Linux with NVIDIA GPU
- USB-C connection or WiFi streaming

### Software
- Python 3.10+
- PyTorch with MPS (macOS) or CUDA (Linux)
- Aria SDK (from Meta)
- See [Setup Guide](docs/setup/SETUP.md) for detailed instructions

---

## ğŸš€ Installation

### macOS (Current)
```bash
# Install dependencies
pip install torch torchvision torchaudio
pip install ultralytics opencv-python numpy projectaria-tools transformers pytest

# Verify TTS
which say  # Should return /usr/bin/say
```

### Linux (Migration Target)
See [NUC Migration Guide](docs/migration/NUC_MIGRATION.md) for CUDA setup.

---

## ğŸ“– Documentation

| Resource | Description |
|----------|-------------|
| [ğŸ“š Documentation Index](docs/INDEX.md) | Central hub for all documentation |
| [ğŸš€ Quick Reference](docs/guides/QUICK_REFERENCE.md) | Common commands and workflows |
| [ğŸ—ï¸ Architecture](docs/architecture/architecture_document.md) | System design and components |
| [ğŸ”§ Setup Guide](docs/setup/SETUP.md) | Detailed installation instructions |
| [ğŸ§ª Testing Guide](docs/testing/README.md) | Test strategy and execution |
| [ğŸ› Troubleshooting](docs/TROUBLESHOOTING.md) | CatÃ¡logo de sÃ­ntomasâ†’acciones |
| [ğŸ¤ Contributing](docs/development/CONTRIBUTING.md) | Workflow, ramas, commits, pruebas |

---

## ğŸ›ï¸ Configuration

Main settings in `src/utils/config.py`:

```python
# Vision
YOLO_DEVICE = "mps"          # GPU device (mps/cuda/cpu)
DEPTH_ENABLED = True          # Enable depth estimation
PERIPHERAL_VISION_ENABLED = True  # SLAM cameras

# Audio
AUDIO_COOLDOWN_SECONDS = 2.0  # Minimum time between commands
BEEP_ENABLED = True           # Distance beeps

# Performance
YOLO_FRAME_SKIP = 3          # Process every Nth frame
DEPTH_FRAME_SKIP = 12        # Depth estimation frequency
```

---

## ğŸ§ª Testing

```bash
# Run full test suite
pytest tests/ -v

# Specific tests
pytest tests/test_navigation_pipeline.py
pytest tests/test_audio_router.py

# With coverage
pytest --cov=src --cov-report=html

# Mock hardware test
python examples/test_mock_basic.py
```

---

## ğŸ“Š Performance

### Current (Linux CUDA + TensorRT) âœ…
- **FPS:** 18-22 fps (RTX 2060)
- **YOLO Latency:** ~40ms (TensorRT FP16)
- **Depth Latency:** ~27ms (ONNX Runtime CUDA)
- **End-to-end:** ~48ms
- **GPU Memory:** ~1.5GB / 6GB

### Optimization Journey
```
v1.0: 3.5 FPS (baseline)
v1.9: 18.4 FPS (+426% with TensorRT/ONNX)
v2.0: 19.0 FPS (+3% with Phase 6 hybrid streams)
```

See [CHANGELOG.md](CHANGELOG.md) for detailed performance history.

---

## ğŸ“ Project Structure

```
aria-nav/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Core system (hardware, vision, audio, navigation)
â”‚   â”œâ”€â”€ presentation/      # UI and visualization
â”‚   â””â”€â”€ utils/             # Configuration and utilities
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ benchmarks/            # Performance benchmarks
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ INDEX.md          # ğŸ“š Start here
â”‚   â”œâ”€â”€ guides/           # User guides
â”‚   â”œâ”€â”€ architecture/     # System design
â”‚   â”œâ”€â”€ development/      # Dev workflows
â”‚   â”œâ”€â”€ migration/        # Platform migration
â”‚   â””â”€â”€ testing/          # Test documentation
â”œâ”€â”€ logs/                  # Runtime logs and telemetry
â””â”€â”€ checkpoints/           # Model weights
```

---

## ğŸ—ºï¸ Roadmap

- [x] RGB pipeline with YOLO + Depth
- [x] Peripheral vision (SLAM cameras)
- [x] Audio routing with priorities
- [x] Web dashboard
- [x] Motion state detection
- [x] TensorRT optimization (YOLO FP16)
- [x] NUC + RTX 2060 migration
- [x] MLflow experiment tracking
- [ ] Multi-language support
- [ ] Mobile companion app
- [ ] Fisheye undistortion optimization

---

## ğŸ¤ Contributing

Contributions are welcome! See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines on:
- Code style and conventions
- Testing requirements
- Pull request process
- Development workflow

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Roberto Rojas Sahuquillo**  
Master's Thesis 2025  
Universidad [Your University]

---

## ğŸ™ Acknowledgments

- Meta Aria team for Project Aria SDK
- Open source community (Ultralytics, Depth-Anything, PyTorch)
- Accessibility research lab

---

## ğŸ“ Support

- ğŸ“š **Documentation:** [docs/INDEX.md](docs/INDEX.md)
- ğŸ› **Issues:** Check [Problem Solving Guide](docs/development/problem_solving_guide.md)
- ğŸ’¬ **Discussions:** Open an issue with `[Question]` tag

---

**Status:** ğŸ”¬ Innovation & Research | ğŸš€ Active Development
