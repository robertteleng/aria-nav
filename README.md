# Aria Navigation System

Sistema de navegaciÃ³n asistida para personas con discapacidad visual usando gafas Meta Aria. El proyecto implementa un pipeline modular que combina visiÃ³n por computador, anÃ¡lisis espacial y comandos de audio con prioridades y cooldown por fuente.

## ğŸ§­ Resumen rÃ¡pido
- âœ… Pipeline RGB modular: `ImageEnhancer` â†’ `DepthEstimator` (MiDaS/Depth-Anything) â†’ `YoloProcessor` (perfiles RGB/SLAM) â†’ `NavigationDecisionEngine`.
- âœ… Audio centralizado: `NavigationAudioRouter` coordina `RgbAudioRouter`, `SlamAudioRouter` y `AudioSystem`, aplica cooldowns dinÃ¡micos y registra mÃ©tricas.
- âœ… VisiÃ³n perifÃ©rica asÃ­ncrona: `SlamDetectionWorker` procesa SLAM1/SLAM2 en paralelo y genera eventos priorizados.
- âœ… PresentaciÃ³n desacoplada: `PresentationManager` + `FrameRenderer` ofrecen dashboard OpenCV/Rerun/Web, overlays de navegaciÃ³n y mini-mapa de profundidad.
- âœ… Suite de pruebas `pytest` cubriendo pipeline, audio router, SLAM, MPS utilities y configuraciones clave.

## ğŸ“š Ãndice
1. [VisiÃ³n general](#visiÃ³n-general)
2. [Arquitectura en breve](#arquitectura-en-breve)
3. [Requisitos](#requisitos)
4. [InstalaciÃ³n](#instalaciÃ³n)
5. [EjecuciÃ³n](#ejecuciÃ³n)
6. [TelemetrÃ­a y observabilidad](#telemetrÃ­a-y-observabilidad)
7. [Estructura del repositorio](#estructura-del-repositorio)
8. [ConfiguraciÃ³n](#configuraciÃ³n)
9. [Flujo de trabajo y pruebas](#flujo-de-trabajo-y-pruebas)
10. [Roadmap](#roadmap)
11. [CrÃ©ditos](#crÃ©ditos)

## VisiÃ³n general
- **Objetivo**: ofrecer navegaciÃ³n asistida en tiempo real aprovechando cÃ¡maras RGB/SLAM e IMU de las Meta Aria.
- **Arquitectura**: `DeviceManager` y `Observer` gestionan el SDK; `Coordinator` orquesta pipeline, audio y SLAM; `PresentationManager` maneja UI; `NavigationAudioRouter` unifica prioridades por fuente.
- **Modularidad**: cada capa estÃ¡ desacoplada para permitir mejoras independientes (hardware â†” visiÃ³n â†” audio â†” presentaciÃ³n â†” telemetrÃ­a).
- DocumentaciÃ³n adicional en `docs/architecture/pipeline_overview.md` y `docs/architecture_document.md`.

## Arquitectura en breve
1. `DeviceManager` configura streaming (USB/Wi-Fi) y obtiene calibraciÃ³n RGB.
2. `Observer` recibe frames RGB/SLAM e IMU, normaliza orientaciÃ³n y estima `motion_state`.
3. `NavigationPipeline` (enhancer + depth + YOLO) produce un `PipelineResult` con timings opcionales.
4. `NavigationDecisionEngine` calcula prioridades; `RgbAudioRouter` formatea mensajes y los envÃ­a al `NavigationAudioRouter`, que decide si hablar vÃ­a `AudioSystem`.
5. Si `PERIPHERAL_VISION_ENABLED` estÃ¡ activo, `SlamDetectionWorker` procesa SLAM1/SLAM2 en background y `SlamAudioRouter` integra sus eventos en el audio centralizado.
6. `PresentationManager` usa `FrameRenderer` y dashboards (OpenCV/Rerun/Web) para overlays RGB, mini-mapa de profundidad, estado de audio y eventos SLAM.

## Requisitos
- **Hardware**
  - Gafas Meta Aria con perfil `profile28` o equivalente habilitado.
  - Mac con macOS 13+ (Apple Silicon recomendado) para modo local.
  - (Opcional) Host remoto (Jetson/Linux) para modo hÃ­brido vÃ­a ImageZMQ.
- **Software**
  - Python 3.10+ con `pip` o Conda/Mamba.
  - Paquetes principales: `torch`, `torchvision`, `ultralytics`, `opencv-python`, `numpy`, `projectaria-tools`, `aria-sdk` (suministrado por Meta), `transformers` (opcional, Depth Anything v2), `pytest`.
  - `say` disponible en macOS para TTS (`which say`).

## InstalaciÃ³n
```bash
# 1. Clonar el repositorio
git clone https://github.com/<tu-usuario>/aria-navigation.git
cd aria-navigation

# 2. Crear entorno (ejemplo con venv; usa Conda si lo prefieres)
python3 -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate
pip install --upgrade pip wheel

# 3. Instalar dependencias principales
pip install torch torchvision torchaudio  # Metal MPS soportado por defecto en macOS
pip install ultralytics opencv-python numpy projectaria-tools transformers pytest
# Instala aria.sdk siguiendo la guÃ­a oficial de Meta Aria (distribuciÃ³n privada).

# 4. (Opcional) Verificar TTS y cÃ¡mara
python -c "import torch; print(torch.__version__)"
which say
```

## EjecuciÃ³n
```bash
# Modo principal con hardware real
python src/main.py

# Modo debug sin hardware (frames sintÃ©ticos + TTS)
python src/main.py debug

# Modo hÃ­brido Mac â†’ Jetson (ImageZMQ sender, procesamiento remoto en desarrollo)
python src/main.py hybrid
```

Controles principales:
- `q`: salir del sistema.
- `t`: prueba del sistema de audio (cola RGB).
- `Ctrl+C`: parada segura gestionada por `CtrlCHandler`.

El arranque pregunta por dashboard (`opencv`, `rerun`, `web`) y habilita el flujo correspondiente. En debug se limitan a OpenCV simplificado.

## TelemetrÃ­a y observabilidad
- `logs/audio_telemetry.jsonl`: `NavigationAudioRouter` registra eventos (`enqueued`, `spoken`, `skipped`, `dropped`) con metadata y resÃºmen de sesiÃ³n.
- `NavigationAudioRouter.get_metrics()`: mÃ©tricas por fuente (RGB, SLAM1, SLAM2), tamaÃ±os de cola y cooldown efectivo.
- `Coordinator`: emite mÃ©tricas `PROFILE` del pipeline (`enhance`, `depth`, `yolo`, `nav_audio`, `render`, `total`) cada `PROFILE_WINDOW_FRAMES`.
- `PresentationManager.log_audio_command()`: historial de comandos reproducidos en la UI.

## Estructura del repositorio
```
aria-navigation/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ meta_stream_all.py
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ audio_telemetry.jsonl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”‚   â”œâ”€â”€ audio_system.py
â”‚   â”‚   â”‚   â””â”€â”€ navigation_audio_router.py
â”‚   â”‚   â”œâ”€â”€ hardware/
â”‚   â”‚   â”‚   â””â”€â”€ device_manager.py
â”‚   â”‚   â”œâ”€â”€ imu/
â”‚   â”‚   â”‚   â””â”€â”€ motion_detector.py
â”‚   â”‚   â”œâ”€â”€ navigation/
â”‚   â”‚   â”‚   â”œâ”€â”€ builder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ coordinator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ navigation_decision_engine.py
â”‚   â”‚   â”‚   â”œâ”€â”€ navigation_pipeline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rgb_audio_router.py
â”‚   â”‚   â”‚   â””â”€â”€ slam_audio_router.py
â”‚   â”‚   â””â”€â”€ vision/
â”‚   â”‚       â”œâ”€â”€ depth_estimator.py
â”‚   â”‚       â”œâ”€â”€ image_enhancer.py
â”‚   â”‚       â”œâ”€â”€ slam_detection_worker.py
â”‚   â”‚       â””â”€â”€ yolo_processor.py
â”‚   â”œâ”€â”€ presentation/
â”‚   â”‚   â”œâ”€â”€ presentation_manager.py
â”‚   â”‚   â””â”€â”€ renderers/frame_renderer.py
â”‚   â””â”€â”€ utils/config.py
â””â”€â”€ tests/
    â””â”€â”€ core/...
```

## ConfiguraciÃ³n
`src/utils/config.py` centraliza los toggles:
- `PERIPHERAL_VISION_ENABLED`, `SLAM_TARGET_FPS`: control de visiÃ³n perifÃ©rica y workers SLAM.
- `YOLO_*`: modelo, dispositivo (MPS), thresholds y frame skipping para perfiles RGB/SLAM.
- `DEPTH_*`, `MIDAS_*`, `DEPTH_ANYTHING_VARIANT`: selecciÃ³n de backend y parÃ¡metros de profundidad.
- `LOW_LIGHT_ENHANCEMENT`, `AUTO_ENHANCEMENT`, `GAMMA_CORRECTION`: estrategia de realce en baja iluminaciÃ³n.
- `ZONE_SYSTEM`, `CENTER_ZONE_*`: definiciÃ³n de zonas y prioridades espaciales.
- `PROFILE_PIPELINE`, `PROFILE_WINDOW_FRAMES`: mÃ©tricas de rendimiento.
- `STREAMING_INTERFACE`, `STREAMING_PROFILE_*`: configuraciÃ³n de streaming Aria (USB/Wi-Fi).

## Flujo de trabajo y pruebas
- `Builder.build_full_system()` crea todas las dependencias (pipeline, audio router, frame renderer, SLAM workers).
- `main_debug()` permite validar integraciÃ³n sin hardware real (frames mock, SLAM sintetizado, TTS).
- Tests unitarios/integraciÃ³n en `tests/` (usar `pytest`). Incluyen pruebas para pipeline, routers RGB/SLAM, audio queue, MPS utils y motion detection.
- RecomendaciÃ³n: tras cambios en cooldowns o thresholds, ejecutar una sesiÃ³n corta y revisar `logs/audio_telemetry.jsonl`.

## Roadmap
- [ ] Empaquetar dependencias (requirements/environment) para instalaciÃ³n reproducible.
- [ ] Completar modo hÃ­brido end-to-end (Mac sender â†” Jetson processor) y compartir telemetrÃ­a.
- [ ] Integrar mÃ©tricas de `NavigationAudioRouter` y profundidad en dashboards interactivos.
- [ ] Documentar troubleshooting de Aria SDK, calibraciones SLAM y requisitos de red.

## CrÃ©ditos
- **Autor**: Roberto Rojas Sahuquillo (TFM 2025).
- **Agradecimientos**: Comunidad Project Aria y colaboradores del laboratorio de accesibilidad.
