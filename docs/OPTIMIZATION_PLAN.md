# üöÄ Plan de Optimizaci√≥n de Performance - RTX 2060

**Objetivo**: Alcanzar 60 FPS con m√°xima calidad en RTX 2060  
**Estado Actual**: 13.6 FPS  
**Gap**: 46.4 FPS (340% mejora necesaria)

---

## üìä An√°lisis Actual del Cuello de Botella

### Tiempos Medidos (por frame)
- **Total latencia**: 66.75ms promedio
- **Frames normales**: ~40-45ms (YOLO + rendering)
- **Frames con depth**: ~90-100ms (+50ms depth overhead)

### Identificaci√≥n de Problemas
1. **üêå GIL de Python**: Threading no paralelo real
2. **üì¶ Streaming**: DDS sample lost (CPU->GPU transfer lento)
3. **üîÑ Sincronizaci√≥n**: Todo secuencial, no hay paralelismo
4. **üíæ Memoria**: Copias innecesarias CPU<->GPU

---

## üéØ Plan de Optimizaci√≥n (5 Fases)

### **FASE 1: Quick Wins - GPU Optimization** ‚ö°
**Objetivo**: 20-25 FPS (mejora 50-80%)  
**Esfuerzo**: Bajo (2-3 horas)  
**Riesgo**: Bajo

#### Acciones:
1. **Batch Processing GPU**
   - [ ] Procesar YOLO y Depth en el mismo batch
   - [ ] Mantener frames en GPU (no bajar a CPU)
   - [ ] Usar `torch.cuda.Stream()` para operaciones paralelas
   
2. **Optimizar Transferencias CPU-GPU**
   - [ ] `pinned_memory=True` para tensores
   - [ ] Reducir `.cpu()` innecesarios
   - [ ] Cache de depth map en GPU
   
3. **Aumentar Batch Size**
   - [ ] YOLO procesar 2-3 frames por inferencia
   - [ ] Depth procesar cada 6-9 frames (vs actual 3)

**C√≥digo a modificar:**
- `src/core/vision/yolo_processor.py`
- `src/core/vision/depth_estimator.py`
- `src/core/navigation/navigation_pipeline.py`

---

### **FASE 2: Multiprocessing - Romper el GIL** üîì
**Objetivo**: 35-40 FPS (mejora 170%)  
**Esfuerzo**: Medio (1-2 d√≠as)  
**Riesgo**: Medio (sincronizaci√≥n)

#### Acciones:
1. **Separar en Procesos Independientes**
   ```
   Process 1: Aria Streaming ‚Üí Queue
   Process 2: YOLO Inference (GPU) ‚Üí Queue
   Process 3: Depth Inference (GPU) ‚Üí Queue
   Process 4: Audio + Rendering (CPU)
   ```

2. **Comunicaci√≥n IPC**
   - [ ] `multiprocessing.Queue` con shared memory
   - [ ] Usar `torch.multiprocessing` para GPU sharing
   - [ ] Pipes para sincronizaci√≥n r√°pida

3. **Shared Memory para Frames**
   - [ ] `shared_memory.SharedMemory` para frames grandes
   - [ ] Evitar pickles pesados

**Nuevo dise√±o:**
```python
src/core/processing/
‚îú‚îÄ‚îÄ aria_capture_process.py    # Streaming loop
‚îú‚îÄ‚îÄ yolo_inference_process.py  # GPU YOLO
‚îú‚îÄ‚îÄ depth_inference_process.py # GPU Depth
‚îî‚îÄ‚îÄ coordinator_process.py     # Main orchestrator
```

---

### **FASE 3: GStreamer Pipeline - Zero-Copy** üìπ
**Objetivo**: 50-55 FPS (mejora 270%)  
**Esfuerzo**: Alto (2-3 d√≠as)  
**Riesgo**: Alto (nueva infraestructura)

#### Acciones:
1. **Reemplazar DDS Streaming con GStreamer**
   - [ ] Pipeline directo Aria ‚Üí GPU memory
   - [ ] Hardware decoding (NVDEC)
   - [ ] Zero-copy con `appsink`

2. **GStreamer Pipeline**
   ```bash
   aria_source ! 
   nvvidconv ! 
   video/x-raw(memory:NVMM) ! 
   appsink
   ```

3. **Integraci√≥n con PyTorch**
   - [ ] CuPy para conversi√≥n NVMM ‚Üí Torch tensor
   - [ ] Direct GPU upload sin CPU pass

**Dependencias nuevas:**
- `gstreamer1.0`
- `gstreamer1.0-plugins-bad`
- `python-gi`
- `cupy-cuda11x`

---

### **FASE 4: Model Optimization - TensorRT** üî•
**Objetivo**: 60+ FPS (mejora 340%+)  
**Esfuerzo**: Alto (3-4 d√≠as)  
**Riesgo**: Medio (conversi√≥n de modelos)

#### Acciones:
1. **YOLO a TensorRT**
   - [ ] Exportar YOLO12n a ONNX
   - [ ] Convertir ONNX ‚Üí TensorRT engine
   - [ ] FP16 precision (2x speed vs FP32)
   - [ ] Dynamic shapes para batch variable

2. **Depth Anything V2 a TensorRT**
   - [ ] Exportar modelo HuggingFace ‚Üí ONNX
   - [ ] TensorRT engine con FP16
   - [ ] Fusi√≥n de capas (layer fusion)

3. **Batch Inferencing**
   - [ ] YOLO: batch=4 frames
   - [ ] Depth: batch=2 frames

**Performance esperado:**
- YOLO: 40-45ms ‚Üí **5-8ms** (5-9x faster)
- Depth: 50ms ‚Üí **10-15ms** (3-5x faster)

**Herramientas:**
- `torch2trt`
- `onnx`
- `tensorrt`

---

### **FASE 5: Advanced Optimizations** üéì
**Objetivo**: 60+ FPS sostenido + features adicionales  
**Esfuerzo**: Medio (1-2 d√≠as)  
**Riesgo**: Bajo

#### Acciones:
1. **Frame Skipping Inteligente**
   - [ ] Skip frames basado en motion detection
   - [ ] Interpolaci√≥n de detecciones entre frames
   - [ ] Predictive tracking

2. **GPU Direct RDMA** (si disponible)
   - [ ] Aria ‚Üí GPU sin pasar por CPU RAM
   - [ ] Requiere hardware espec√≠fico

3. **Async Everything**
   - [ ] CUDA streams para overlap
   - [ ] Async audio processing
   - [ ] Non-blocking rendering

4. **Model Quantization**
   - [ ] INT8 inference donde sea posible
   - [ ] Mixed precision (FP16/FP32)

---

## üìã Plan de Ejecuci√≥n Recomendado

### ‚úÖ Semana 1: Fundamentos (COMPLETADO)
- **‚úÖ D√≠a 1-2**: FASE 1 (Quick Wins) - Completado
  - Optimizaciones CUDA, pinned memory, streams paralelos
  - Resoluciones aumentadas: YOLO 640px, Depth 384px
- **‚úÖ D√≠a 3-5**: FASE 2 (Multiprocessing) - Completado
  - Workers GPU spawn, 20.19 FPS sin depth
  - **BONUS**: Depth-Anything-V2 GPU-optimizado (80ms‚Üí10ms, 8x speedup)
  - Commit: fa642c0

### ‚è≥ Semana 2: Validaci√≥n + Infraestructura (EN PROGRESO)
- **‚è≥ D√≠a 1**: Testing con depth integrado
  - Medir FPS real con depth+YOLO paralelo
  - Benchmark 50-200 frames, stress test 10min
  - Validar latency <100ms, FPS ‚â•15
- **‚è≥ D√≠a 2-3**: FASE 3 (GStreamer) - OPCIONAL
  - Evaluar si streaming es bottleneck
  - Solo si FPS <15 por problemas de transferencia
- **‚è≥ D√≠a 4-5**: Documentaci√≥n intermedia
  - Actualizar FASE_2_IMPLEMENTATION.md con depth
  - Field notes con resultados

### üéØ Semana 3: Aceleraci√≥n TensorRT (SIGUIENTE)
- **D√≠a 1-2**: Conversi√≥n YOLO12n ‚Üí TensorRT
  - Export ONNX, TensorRT engine FP16
  - Benchmark mejora esperada: 40ms‚Üí5-8ms
- **D√≠a 3-4**: Conversi√≥n Depth-Anything-V2 ‚Üí TensorRT
  - Export PyTorch‚ÜíONNX‚ÜíTensorRT
  - Benchmark mejora esperada: 10ms‚Üí3-5ms
- **D√≠a 5**: FASE 5 (Advanced) + Testing final
  - INT8 quantization si es necesario
  - Validaci√≥n completa sistema

---

## üîß Configuraci√≥n Hardware √ìptima (RTX 2060)

```python
# Config para RTX 2060 (6GB VRAM)
YOLO_IMAGE_SIZE = 640           # Max resolution
YOLO_BATCH_SIZE = 4             # 4 frames simult√°neos
YOLO_FP16 = True                # Half precision
DEPTH_INPUT_SIZE = 384          # Alta resoluci√≥n
DEPTH_BATCH_SIZE = 2            # 2 frames simult√°neos
DEPTH_FP16 = True               # Half precision

# CUDA optimizations
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.set_float32_matmul_precision('high')

# Memory management
torch.cuda.empty_cache()        # Peri√≥dicamente
CUDA_VISIBLE_DEVICES = "0"      # Single GPU
```

---

## üìà Mejoras Esperadas por Fase

| Fase | FPS | Mejora | Esfuerzo | Prioridad |
|------|-----|--------|----------|-----------|
| Actual | 13.6 | - | - | - |
| FASE 1 | 22-25 | +65% | Bajo | ‚≠ê‚≠ê‚≠ê |
| FASE 2 | 35-40 | +170% | Medio | ‚≠ê‚≠ê‚≠ê |
| FASE 3 | 50-55 | +270% | Alto | ‚≠ê‚≠ê |
| FASE 4 | 60+ | +340% | Alto | ‚≠ê‚≠ê‚≠ê |
| FASE 5 | 60+ | - | Medio | ‚≠ê |

---

## üö¶ Empezar con FASE 1 - Quick Wins

### Implementaci√≥n Inmediata:

1. **Aumentar resoluciones**
   ```python
   YOLO_IMAGE_SIZE = 640
   DEPTH_INPUT_SIZE = 384
   ```

2. **Optimizar frame skipping**
   ```python
   YOLO_FRAME_SKIP = 0      # Procesar todo
   DEPTH_FRAME_SKIP = 6     # Reducir overhead
   ```

3. **GPU pinned memory**
   ```python
   frame_tensor = torch.from_numpy(frame).cuda(non_blocking=True)
   ```

4. **CUDA streams**
   ```python
   yolo_stream = torch.cuda.Stream()
   depth_stream = torch.cuda.Stream()
   ```

---

## üìù Tracking de Progreso

- [x] **FASE 1: Quick Wins** (Target: 22-25 FPS) ‚úÖ COMPLETADO
  - [x] CUDA streams paralelos (depth + YOLO)
  - [x] Pinned memory para transfers
  - [x] Resoluciones optimizadas (640px YOLO, 384px Depth)
  - [x] Optimizaciones cuDNN/TF32
  - **Resultado**: Base s√≥lida para multiprocessing

- [x] **FASE 2: Multiprocessing** (Target: 35-40 FPS) ‚úÖ COMPLETADO
  - [x] Workers GPU con spawn method
  - [x] Central worker (depth+YOLO) + SLAM workers
  - [x] IPC con multiprocessing.Queue
  - [x] **BONUS**: Depth-Anything-V2 GPU-optimized (8x speedup)
  - **Resultado**: 20.19 FPS sin depth, ~15-18 FPS esperado con depth

- [ ] **FASE 3: GStreamer** (Target: 50-55 FPS) ‚è∏Ô∏è EVALUACI√ìN
  - [ ] An√°lisis de bottleneck en streaming
  - [ ] Zero-copy pipeline si es necesario
  - [ ] Hardware decoding NVDEC
  - **Estado**: Evaluar despu√©s de tests con depth

- [ ] **FASE 4: TensorRT** (Target: 60+ FPS) üéØ SIGUIENTE
  - [ ] YOLO12n ‚Üí TensorRT FP16
  - [ ] Depth-Anything-V2 ‚Üí TensorRT FP16
  - [ ] Batch inferencing optimizado
  - [ ] Benchmarking comparativo
  - **Aprendizaje esperado**: ONNX export, TensorRT API, FP16 optimization

- [ ] **FASE 5: Advanced** (Target: 60+ FPS sostenido) üîÆ FUTURO
  - [ ] INT8 quantization
  - [ ] Frame skipping inteligente
  - [ ] CUDA kernel fusion
  - [ ] Profiling avanzado

---

## üéØ ¬øPor d√≥nde empezamos?

**Estado Actual**: FASE 1 y 2 completadas. Depth-Anything-V2 integrado y optimizado.

**Pr√≥ximos Pasos Inmediatos:**
1. ‚úÖ **Test completo con depth** (hoy/ma√±ana)
2. ‚úÖ **Stress testing 10min** (validar estabilidad)
3. üìä **An√°lisis de resultados** (decidir si FASE 3 necesaria)
4. üöÄ **FASE 4: TensorRT** (pr√≥xima gran milestone)

---

## üìö Roadmap de Aprendizaje - FASE 4 (TensorRT)

### Conceptos a Dominar

#### 1. **ONNX (Open Neural Network Exchange)**
**Qu√© es**: Formato intermedio para intercambiar modelos entre frameworks
```
PyTorch ‚Üí ONNX ‚Üí TensorRT
```

**Por qu√© importa**: 
- TensorRT no lee PyTorch directamente
- ONNX permite portabilidad (PyTorch, TensorFlow, etc.)
- Validaci√≥n de compatibilidad de operaciones

**Aprender√°s**:
- Export de modelos PyTorch a ONNX
- Dynamic shapes vs static shapes
- Debugging de exportaci√≥n (ops no soportadas)
- Simplificaci√≥n de grafos con `onnx-simplifier`

**Recursos**:
- [PyTorch ONNX Export](https://pytorch.org/docs/stable/onnx.html)
- [ONNX Operator Coverage](https://onnx.ai/onnx/operators/)

---

#### 2. **TensorRT Fundamentals**
**Qu√© es**: Motor de inference optimizado de NVIDIA

**Aprender√°s**:
- **Builder**: Crear engine desde ONNX
- **Engine**: Modelo compilado optimizado para GPU espec√≠fica
- **Context**: Runtime execution environment
- **Precision modes**: FP32, FP16, INT8
- **Layer fusion**: Combinar operaciones para reducir latency
- **Kernel auto-tuning**: TensorRT prueba m√∫ltiples implementaciones

**Pipeline TensorRT**:
```python
# 1. Parse ONNX
parser.parse(onnx_model)

# 2. Build con optimizaciones
config.set_flag(trt.BuilderFlag.FP16)
engine = builder.build_engine(network, config)

# 3. Inference
context = engine.create_execution_context()
context.execute_v2(bindings)
```

**Recursos**:
- [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/)
- [TensorRT Python API](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/)

---

#### 3. **Precision Optimization**

**FP32 ‚Üí FP16 (Half Precision)**:
- **Speedup**: 2x t√≠picamente (RTX 2060 tiene Tensor Cores FP16)
- **Trade-off**: M√≠nima p√©rdida de precisi√≥n (~0.1% accuracy)
- **Cu√°ndo usar**: Siempre como primer paso

**FP16 ‚Üí INT8 (Quantization)**:
- **Speedup**: 4x potencialmente
- **Trade-off**: Puede afectar accuracy (~1-3%)
- **Requiere**: Calibration dataset
- **Cu√°ndo usar**: Si FP16 no alcanza FPS target

**Aprender√°s**:
- Calibration para INT8
- Post-training quantization (PTQ)
- Quantization-aware training (QAT) - avanzado

---

#### 4. **Dynamic Shapes & Batching**

**Problema**: Tama√±o de entrada variable
```python
# Batch size puede variar: 1, 2, 4 frames
input_shape = (batch_size, 3, 640, 640)  # Variable
```

**Soluci√≥n TensorRT**:
```python
# Optimization profiles
profile.set_shape("input", 
    min=(1,3,640,640),
    opt=(2,3,640,640),  # Optimal
    max=(4,3,640,640))
```

**Aprender√°s**:
- Optimization profiles
- Trade-off: flexibilidad vs performance
- Cu√°ndo usar static shapes (mejor performance)

---

#### 5. **Debugging & Profiling**

**Tools que usar√°s**:
- `trtexec`: CLI para testing r√°pido
- `Nsight Systems`: Profiling GPU end-to-end
- `polygraphy`: Debugging TensorRT conversions
- `onnx-graphsurgeon`: Modificar grafos ONNX

**Aprender√°s**:
- Identificar layers lentas
- Comparar accuracy PyTorch vs TensorRT
- Debugging de conversi√≥n fallida
- Memory profiling

---

### Plan de Implementaci√≥n Detallado

#### **Milestone 1: YOLO12n ‚Üí TensorRT** (2 d√≠as)

**D√≠a 1 - Export & Build**:
```bash
# 1. Export PyTorch ‚Üí ONNX
python export_yolo_onnx.py

# 2. Simplify ONNX
onnxsim yolo12n.onnx yolo12n_simplified.onnx

# 3. Build TensorRT engine
trtexec --onnx=yolo12n.onnx \
        --fp16 \
        --workspace=2048 \
        --saveEngine=yolo12n_fp16.trt
```

**D√≠a 2 - Integration & Benchmark**:
```python
# Integrar en yolo_processor.py
class YoloProcessorTRT:
    def __init__(self):
        self.engine = load_trt_engine("yolo12n_fp16.trt")
        self.context = self.engine.create_execution_context()
    
    def process_frame(self, frame):
        # Inference con TensorRT
        pass
```

**Validaci√≥n**:
- Accuracy: Compare detections PyTorch vs TRT (debe ser ~99% igual)
- Performance: Measure latency improvement
- Target: 40ms ‚Üí 5-8ms

---

#### **Milestone 2: Depth-Anything-V2 ‚Üí TensorRT** (2 d√≠as)

**D√≠a 1 - Export Custom Model**:
```python
# M√°s complejo que YOLO (modelo custom)
# Puede requerir modificaciones al grafo

# 1. Trace model
traced = torch.jit.trace(depth_model, example_input)

# 2. Export to ONNX
torch.onnx.export(traced, ...)

# 3. Fix incompatible ops si es necesario
```

**D√≠a 2 - Optimization & Integration**:
```python
# Integrar en central_worker.py
# Reemplazar infer_image_gpu() con TRT version
```

**Validaci√≥n**:
- Depth map comparison (visual + MSE)
- Performance: 10-11ms ‚Üí 3-5ms esperado

---

#### **Milestone 3: System Integration** (1 d√≠a)

**Testing Completo**:
- Benchmark end-to-end: YOLO TRT + Depth TRT
- Stress test 10min con TensorRT
- Memory profiling
- FPS target: 40-60 FPS esperado

**Rollback Plan**:
- Keep PyTorch versions como fallback
- Feature flag para switch entre PyTorch/TRT
```python
USE_TENSORRT = os.getenv("USE_TENSORRT", "true") == "true"
```

---

### Recursos de Aprendizaje

**Documentaci√≥n Oficial**:
- [TensorRT Quick Start Guide](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/)
- [TensorRT Best Practices](https://docs.nvidia.com/deeplearning/tensorrt/best-practices/)
- [ONNX Tutorials](https://onnx.ai/onnx/intro/)

**Ejemplos Pr√°cticos**:
- [TensorRT GitHub Samples](https://github.com/NVIDIA/TensorRT/tree/main/samples)
- [YOLOv8 TensorRT Example](https://github.com/triple-Mu/YOLOv8-TensorRT)
- [torch2trt Examples](https://github.com/NVIDIA-AI-IOT/torch2trt)

**Community**:
- [NVIDIA Developer Forums - TensorRT](https://forums.developer.nvidia.com/c/ai/tensorrt/)
- r/computervision subreddit
- Stack Overflow `[tensorrt]` tag

---

### Checklist Pre-TensorRT

Antes de empezar FASE 4, asegurarte de:
- [ ] PyTorch models funcionan correctamente
- [ ] Benchmarks baseline documentados (latency, accuracy)
- [ ] Test dataset preparado para validaci√≥n
- [ ] TensorRT instalado y funcionando (`trtexec --help`)
- [ ] Familiarizado con ONNX export b√°sico

---

### Expected Challenges & Solutions

**Challenge 1: Unsupported ONNX Ops**
```
Error: Unsupported operator 'CustomOp'
```
**Solution**: 
- Check ONNX operator support
- Rewrite op usando ops soportados
- Use `torch.onnx.register_custom_op_symbolic()`

**Challenge 2: Accuracy Degradation**
```
TensorRT predictions differ significantly
```
**Solution**:
- Compare intermediate outputs layer-by-layer
- Use FP32 first, then FP16
- Adjust calibration for INT8

**Challenge 3: Dynamic Shapes Issues**
```
Error: Input shape does not match
```
**Solution**:
- Use optimization profiles
- Consider static shapes for best performance

---

## üéì ¬øListo para FASE 4?

**Primero**: Completa testing actual (depth integrado)
**Cuando est√©s listo**: Seguiremos esta gu√≠a paso a paso
**Objetivo final**: Sistema completo optimizado + conocimiento profundo de inference optimization
