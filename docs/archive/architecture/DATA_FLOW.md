# ğŸŒŠ Data Flow Analysis: Frame-by-Frame Journey

> **Detailed trace of how data flows through the Aria Navigation System**
> Last updated: November 25, 2025

## Overview

This document traces a single RGB frame from capture to audio output, showing timing, transformations, and decision points.

---

## Timeline: Frame N = 180 (Example at 60 FPS, 3 seconds in)

```
Time (ms)  | Component              | Operation                      | Data Shape
-----------|------------------------|--------------------------------|------------------
0          | Aria SDK               | Capture RGB frame              | (480, 640, 3) uint8
2          | Observer               | Undistortion (fisheye)         | (480, 640, 3) uint8
4          | Coordinator            | Route to RGB pipeline          | (480, 640, 3) uint8
6          | ImageEnhancer          | Brightness/contrast adjust     | (480, 640, 3) uint8
8          | NavigationPipeline     | Check frame skip (180 % 3 == 0)| YES â†’ run YOLO
10         | YOLOProcessor          | Resize to 640x640              | (640, 640, 3) uint8
12         | YOLOProcessor          | HWC â†’ BCHW, normalize          | (1, 3, 640, 640) float32
14         | TensorRT Engine        | GPU inference (FP16)           | 40ms latency
54         | YOLOProcessor          | NMS post-processing            | List[Detection]
56         | NavigationPipeline     | Check depth skip (180 % 12 == 0)| YES â†’ run Depth
58         | DepthEstimator         | Resize to 518x518              | (518, 518, 3) uint8
60         | DepthEstimator         | Normalize to [0, 1]            | (518, 518, 3) float32
62         | DepthEstimator         | HWC â†’ BCHW                     | (1, 3, 518, 518)
64         | ONNX Runtime (CUDA)    | GPU inference                  | 27ms latency
91         | DepthEstimator         | Resize depth map back          | (480, 640) float32
93         | NavigationPipeline     | Fuse depth with detections     | List[Detection+Depth]
95         | ObjectTracker          | Update tracks                  | List[TrackedObject]
97         | NavigationPipeline     | Classify zones (L/C/R)         | List[TrackedObject+Zone]
99         | DecisionEngine         | Calculate priorities           | List[Priority]
101        | DecisionEngine         | Generate top command           | "Close person center"
103        | AudioRouter            | Check cooldown (2s)            | PASS
105        | AudioRouter            | Enqueue TTS command            | Queue.put()
107        | AudioSystem            | TTS synthesis                  | WAV buffer
150        | AudioSystem            | Play audio to speaker          | User hears alert
```

**Total Latency:** 107ms (capture â†’ decision)
**Audio Latency:** +43ms (TTS synthesis)
**End-to-End:** 150ms

---

## Detailed Flow Diagrams

### 1. Frame Acquisition (Observer Layer)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ARIA GLASSES                        â”‚
â”‚  â€¢ RGB Camera: 640x480 @ 60fps                â”‚
â”‚  â€¢ Fisheye lens (180Â° FOV)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ USB-C / WiFi stream
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AriaObserver.get_rgb_frame()         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Poll Aria SDK                             â”‚
â”‚     â””â”€ aria_sdk.get_image_data()              â”‚
â”‚  2. Check if new frame available              â”‚
â”‚     â””â”€ if timestamp > last_timestamp          â”‚
â”‚  3. Undistort fisheye (optional)              â”‚
â”‚     â””â”€ cv2.undistort() using calibration      â”‚
â”‚  4. Rotate 90Â° CCW (device orientation)       â”‚
â”‚     â””â”€ np.rot90(frame, -1)                    â”‚
â”‚  5. Return contiguous array                   â”‚
â”‚     â””â”€ np.ascontiguousarray(frame)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ RGB frame ready
                 â–¼
         Coordinator.run()
```

**Key Transformations:**
1. **Fisheye â†’ Rectified:** Remove lens distortion using Aria calibration
2. **Rotation:** Aria SDK returns portrait, we need landscape
3. **Contiguous Memory:** Required for YOLO (avoids copy)

**Timing:**
- SDK fetch: <1ms (already buffered)
- Undistortion: ~2ms (if enabled)
- Rotation: <1ms (view operation)

---

### 2. Image Enhancement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ImageEnhancer.enhance(frame)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: (480, 640, 3) uint8 RGB               â”‚
â”‚                                                â”‚
â”‚  1. Convert to HSV color space                â”‚
â”‚     â””â”€ cv2.cvtColor(frame, cv2.COLOR_RGB2HSV) â”‚
â”‚                                                â”‚
â”‚  2. Brightness adjustment                     â”‚
â”‚     â””â”€ H, S, V = hsv[:,:,0], hsv[:,:,1], hsv[:,:,2] â”‚
â”‚     â””â”€ V = V * brightness_factor (1.2)        â”‚
â”‚                                                â”‚
â”‚  3. Contrast normalization                    â”‚
â”‚     â””â”€ V = (V - V.mean()) * contrast + V.mean() â”‚
â”‚                                                â”‚
â”‚  4. Convert back to RGB                       â”‚
â”‚     â””â”€ cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)   â”‚
â”‚                                                â”‚
â”‚  Output: (480, 640, 3) uint8 RGB (enhanced)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why enhance?**
- Meta Aria cameras optimized for recording, not real-time CV
- Low-light conditions common in indoor navigation
- Improves YOLO detection confidence by 5-10%

**Configuration:** `Config.ENHANCEMENT_ENABLED = True`

---

### 3. YOLO Detection (TensorRT)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        YOLOProcessor.detect(frame)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: (480, 640, 3) uint8 RGB               â”‚
â”‚                                                â”‚
â”‚  [Preprocessing - 2ms]                        â”‚
â”‚  1. Resize to square                          â”‚
â”‚     â””â”€ cv2.resize(frame, (640, 640))          â”‚
â”‚  2. Normalize to [0, 1]                       â”‚
â”‚     â””â”€ frame_float = frame.astype(float32) / 255.0 â”‚
â”‚  3. Transpose HWC â†’ CHW                       â”‚
â”‚     â””â”€ chw = np.transpose(hwc, (2, 0, 1))     â”‚
â”‚  4. Add batch dimension                       â”‚
â”‚     â””â”€ bchw = chw[None, ...]  # (1, 3, 640, 640) â”‚
â”‚                                                â”‚
â”‚  [TensorRT Inference - 40ms]                  â”‚
â”‚  5. Copy to GPU                               â”‚
â”‚     â””â”€ input_buffer = cuda.memcpy_htod(bchw)  â”‚
â”‚  6. Execute engine                            â”‚
â”‚     â””â”€ context.execute_v2(bindings)           â”‚
â”‚  7. Copy outputs to CPU                       â”‚
â”‚     â””â”€ boxes, scores, classes = fetch_outputs() â”‚
â”‚                                                â”‚
â”‚  [Post-processing - 3ms]                      â”‚
â”‚  8. Non-Maximum Suppression (NMS)             â”‚
â”‚     â””â”€ Filter overlapping boxes (IoU > 0.45)  â”‚
â”‚  9. Confidence filtering                      â”‚
â”‚     â””â”€ Keep only scores > 0.5                 â”‚
â”‚  10. Create DetectedObject instances          â”‚
â”‚     â””â”€ [DetectedObject(...) for each box]     â”‚
â”‚                                                â”‚
â”‚  Output: List[DetectedObject]                 â”‚
â”‚          â””â”€ class_id, confidence, bbox, ...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TensorRT Optimizations:**
- **FP16 Precision:** 2x faster than FP32, minimal accuracy loss
- **Layer Fusion:** Convolution + BatchNorm + ReLU â†’ single kernel
- **Dynamic Shapes:** Disabled (fixed 640x640 for optimal performance)

**Detection Format:**
```python
@dataclass
class DetectedObject:
    class_id: int          # COCO class (0-79)
    class_name: str        # "person", "car", etc.
    confidence: float      # 0.0 - 1.0
    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)
    timestamp: float       # Frame capture time
    zone: str              # "left", "center", "right" (added later)
    distance: float        # meters (added by depth fusion)
    distance_bucket: str   # "close", "medium", "far"
    track_id: Optional[int] # Tracking ID (added by tracker)
```

---

### 4. Depth Estimation (ONNX)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     DepthEstimator.estimate(frame)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: (480, 640, 3) uint8 RGB               â”‚
â”‚                                                â”‚
â”‚  [Preprocessing - 3ms]                        â”‚
â”‚  1. Resize to model input size                â”‚
â”‚     â””â”€ resized = cv2.resize(frame, (518, 518)) â”‚
â”‚  2. Normalize to [0, 1]                       â”‚
â”‚     â””â”€ normalized = resized.astype(float32) / 255.0 â”‚
â”‚  3. Transpose HWC â†’ CHW                       â”‚
â”‚     â””â”€ chw = np.transpose(hwc, (2, 0, 1))     â”‚
â”‚  4. Add batch dimension                       â”‚
â”‚     â””â”€ bchw = chw[None, ...]  # (1, 3, 518, 518) â”‚
â”‚                                                â”‚
â”‚  [ONNX Runtime Inference - 27ms]              â”‚
â”‚  5. Run session (CUDA Execution Provider)     â”‚
â”‚     â””â”€ output = session.run(["depth"], {"image": bchw}) â”‚
â”‚  6. Squeeze batch dimension                   â”‚
â”‚     â””â”€ depth_map = output[0][0]  # (518, 518)  â”‚
â”‚                                                â”‚
â”‚  [Post-processing - 2ms]                      â”‚
â”‚  7. Resize to original frame size             â”‚
â”‚     â””â”€ depth_resized = cv2.resize(depth_map,  â”‚
â”‚                                    (640, 480)) â”‚
â”‚  8. Normalize to [0, 255] for visualization   â”‚
â”‚     â””â”€ depth_vis = (depth_map / depth_map.max()) * 255 â”‚
â”‚                                                â”‚
â”‚  Output: (480, 640) float32 inverse depth     â”‚
â”‚          â””â”€ Closer objects = higher values    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Model Architecture:** Depth-Anything v2 Small
- **Backbone:** DINOv2 (Vision Transformer)
- **Decoder:** Dense Prediction Transformer (DPT)
- **Training:** MiDaS-style relative depth

**Inverse Depth:**
- Model outputs inverse depth: `d_inv = 1 / d_real`
- Why? More stable gradients during training
- Convert to real depth: `d_real = 1 / d_inv`

---

### 5. Depth-Detection Fusion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NavigationPipeline._fuse_depth(dets, depth_map) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  For each detection:                           â”‚
â”‚                                                â”‚
â”‚  1. Extract bounding box coordinates          â”‚
â”‚     â””â”€ x1, y1, x2, y2 = det.bbox              â”‚
â”‚                                                â”‚
â”‚  2. Crop depth map to bbox region             â”‚
â”‚     â””â”€ bbox_depth = depth_map[y1:y2, x1:x2]   â”‚
â”‚                                                â”‚
â”‚  3. Calculate statistics                      â”‚
â”‚     â””â”€ mean_inv_depth = np.mean(bbox_depth)   â”‚
â”‚     â””â”€ std_inv_depth = np.std(bbox_depth)     â”‚
â”‚                                                â”‚
â”‚  4. Convert to real depth (meters)            â”‚
â”‚     â””â”€ distance = 1.0 / (mean_inv_depth + 1e-6) â”‚
â”‚                                                â”‚
â”‚  5. Classify distance bucket                  â”‚
â”‚     â””â”€ if distance < 2.0: bucket = "close"    â”‚
â”‚        elif distance < 5.0: bucket = "medium" â”‚
â”‚        else: bucket = "far"                   â”‚
â”‚                                                â”‚
â”‚  6. Update detection object                   â”‚
â”‚     â””â”€ det.distance = distance                â”‚
â”‚        det.distance_bucket = bucket           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Challenges:**
1. **Occlusion:** Depth map includes background behind transparent objects
   - **Solution:** Use median instead of mean for robust estimate
2. **Scale ambiguity:** Relative depth (not absolute)
   - **Solution:** Calibrate buckets empirically (2m, 5m thresholds)

---

### 6. Object Tracking

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ObjectTracker.update(detections)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  State: self.tracks = {track_id: Track(...)}  â”‚
â”‚                                                â”‚
â”‚  1. Match current detections to existing tracksâ”‚
â”‚     â””â”€ For each detection:                    â”‚
â”‚         â€¢ Calculate IoU with all tracks       â”‚
â”‚         â€¢ If IoU > 0.3: assign to track       â”‚
â”‚         â€¢ Else: create new track              â”‚
â”‚                                                â”‚
â”‚  2. Update matched tracks                     â”‚
â”‚     â””â”€ track.update(detection)                â”‚
â”‚        â€¢ Kalman filter prediction (optional)  â”‚
â”‚        â€¢ Update bbox, confidence              â”‚
â”‚        â€¢ Increment hit counter                â”‚
â”‚                                                â”‚
â”‚  3. Handle unmatched tracks                   â”‚
â”‚     â””â”€ track.miss_count += 1                  â”‚
â”‚        if track.miss_count > 5:               â”‚
â”‚            delete track (object left frame)   â”‚
â”‚                                                â”‚
â”‚  4. Handle new detections                     â”‚
â”‚     â””â”€ Create Track(id=next_id, ...)          â”‚
â”‚        self.tracks[next_id] = track           â”‚
â”‚        next_id += 1                            â”‚
â”‚                                                â”‚
â”‚  5. Return tracked detections                 â”‚
â”‚     â””â”€ [det with track_id assigned]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tracking Algorithm:** IoU-based (simple but effective)
- **IoU = Intersection over Union:** Overlap area / Total area
- **Threshold:** 0.3 (tuned empirically)

**Why track?**
- Reduce flickering (same object gets same ID across frames)
- Enable temporal reasoning ("person was here 5 frames ago")
- Future: Predict motion trajectories

---

### 7. Spatial Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NavigationPipeline._classify_zone(bbox)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: bbox = (x1, y1, x2, y2)               â”‚
â”‚         frame_width = 640                     â”‚
â”‚                                                â”‚
â”‚  1. Calculate bbox center                     â”‚
â”‚     â””â”€ center_x = (x1 + x2) / 2               â”‚
â”‚                                                â”‚
â”‚  2. Define zone boundaries                    â”‚
â”‚     â””â”€ left_boundary = frame_width * 0.33     â”‚
â”‚        right_boundary = frame_width * 0.66    â”‚
â”‚                                                â”‚
â”‚  3. Classify based on center_x                â”‚
â”‚     â””â”€ if center_x < left_boundary:           â”‚
â”‚            zone = "left"                      â”‚
â”‚        elif center_x > right_boundary:        â”‚
â”‚            zone = "right"                     â”‚
â”‚        else:                                  â”‚
â”‚            zone = "center"                    â”‚
â”‚                                                â”‚
â”‚  Output: "left" | "center" | "right"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Zone Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEFT  â”‚   CENTER   â”‚  RIGHT     â”‚
â”‚ 0-33% â”‚   33-66%   â”‚  66-100%   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       â”‚            â”‚            â”‚
â”‚   ğŸš¶  â”‚     ğŸš—     â”‚            â”‚
â”‚       â”‚            â”‚     ğŸš²     â”‚
â”‚       â”‚            â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why 3 zones?**
- Simple audio commands: "person left" vs "person right"
- User can turn head to center the object
- Balance: more zones = cognitive overload

---

### 8. Decision Engine (Priority Calculation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DecisionEngine.process_detections(rgb, slam)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Combine RGB + SLAM detections             â”‚
â”‚     â””â”€ all_dets = rgb_dets + slam_dets        â”‚
â”‚                                                â”‚
â”‚  2. Calculate priority for each               â”‚
â”‚     â””â”€ For det in all_dets:                   â”‚
â”‚         priority = 0                          â”‚
â”‚         # Distance factor                     â”‚
â”‚         if det.distance_bucket == "close":    â”‚
â”‚             priority += 100                   â”‚
â”‚         elif det.distance_bucket == "medium": â”‚
â”‚             priority += 50                    â”‚
â”‚         else:                                 â”‚
â”‚             priority += 10                    â”‚
â”‚                                                â”‚
â”‚         # Zone factor                         â”‚
â”‚         if det.zone == "center":              â”‚
â”‚             priority += 30                    â”‚
â”‚         else:                                 â”‚
â”‚             priority += 10                    â”‚
â”‚                                                â”‚
â”‚         # Class factor                        â”‚
â”‚         if det.class_name in DANGEROUS:       â”‚
â”‚             priority += 40                    â”‚
â”‚         elif det.class_name in MOVING:        â”‚
â”‚             priority += 20                    â”‚
â”‚                                                â”‚
â”‚         # Motion state                        â”‚
â”‚         if self.motion_state == "stationary": â”‚
â”‚             priority *= 0.5                   â”‚
â”‚                                                â”‚
â”‚         det.priority = priority               â”‚
â”‚                                                â”‚
â”‚  3. Sort by priority (descending)             â”‚
â”‚     â””â”€ sorted_dets = sorted(all_dets,         â”‚
â”‚                             key=lambda d: d.priority, â”‚
â”‚                             reverse=True)     â”‚
â”‚                                                â”‚
â”‚  4. Generate command for top detection        â”‚
â”‚     â””â”€ top_det = sorted_dets[0]               â”‚
â”‚        command = self._generate_command(top_det) â”‚
â”‚                                                â”‚
â”‚  5. Send to audio router                      â”‚
â”‚     â””â”€ self.audio_router.route_command(command) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Priority Formula:**

```
Priority = Distance Factor + Zone Factor + Class Factor

Where:
  Distance Factor = {100 (close), 50 (medium), 10 (far)}
  Zone Factor = {30 (center), 10 (left/right)}
  Class Factor = {40 (dangerous), 20 (moving), 0 (static)}

If user stationary: Priority *= 0.5
```

**Example Scenarios:**

| Detection | Distance | Zone | Class | Priority | Reason |
|-----------|----------|------|-------|----------|--------|
| Car | close | center | car | 170 | High threat |
| Person | medium | left | person | 80 | Medium threat |
| Chair | far | right | chair | 20 | Low priority |

---

### 9. Audio Routing (Cooldown Management)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AudioRouter.route_command(command)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  State: self.last_command_time = {}           â”‚
â”‚         self.audio_queue = Queue()            â”‚
â”‚                                                â”‚
â”‚  1. Check if command should be sent           â”‚
â”‚     â””â”€ now = time.time()                      â”‚
â”‚        last_time = self.last_command_time.get(command, 0) â”‚
â”‚        elapsed = now - last_time              â”‚
â”‚                                                â”‚
â”‚        if elapsed < AUDIO_COOLDOWN_SECONDS:   â”‚
â”‚            return  # Skip (too soon)          â”‚
â”‚                                                â”‚
â”‚  2. Update last command time                  â”‚
â”‚     â””â”€ self.last_command_time[command] = now  â”‚
â”‚                                                â”‚
â”‚  3. Enqueue command                           â”‚
â”‚     â””â”€ self.audio_queue.put(command)          â”‚
â”‚                                                â”‚
â”‚  4. Trigger audio system (separate thread)    â”‚
â”‚     â””â”€ Audio thread pulls from queue          â”‚
â”‚        Synthesizes TTS                        â”‚
â”‚        Plays to speaker                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Cooldown Rationale:**
- **Without cooldown:** 18 commands/sec â†’ audio spam
- **With 2s cooldown:** Max 0.5 commands/sec â†’ digestible
- **Per-command cooldown:** "car left" and "person right" can interleave

**Configuration:**
```python
Config.AUDIO_COOLDOWN_SECONDS = 2.0  # Tunable
```

---

### 10. Audio System (TTS Synthesis)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     AudioSystem._audio_worker_thread()         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Runs in background daemon thread             â”‚
â”‚                                                â”‚
â”‚  while not self.stop_event.is_set():          â”‚
â”‚      1. Block on queue (wait for command)     â”‚
â”‚         â””â”€ command = self.queue.get(timeout=1) â”‚
â”‚                                                â”‚
â”‚      2. Synthesize TTS                        â”‚
â”‚         â””â”€ self.tts_engine.say(command)       â”‚
â”‚            self.tts_engine.runAndWait()       â”‚
â”‚            # Blocking call (~43ms for 3 words) â”‚
â”‚                                                â”‚
â”‚      3. Play to speaker                       â”‚
â”‚         â””â”€ Audio output via system API        â”‚
â”‚            (ALSA on Linux, CoreAudio on macOS) â”‚
â”‚                                                â”‚
â”‚      4. Mark task done                        â”‚
â”‚         â””â”€ self.queue.task_done()             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TTS Engine:** pyttsx3 (wrapper for espeak/sapi5)
- **Speed:** 150 WPM (tunable)
- **Voice:** System default (can select)
- **Latency:** ~43ms for short commands

**Alternative TTS:**
- **Google Cloud TTS:** Higher quality, requires internet
- **Coqui TTS:** Local neural TTS, higher latency
- **Current:** pyttsx3 for low-latency local TTS

---

## Performance Analysis

### Latency Breakdown (Frame 180)

| Stage | Duration (ms) | % of Total | Parallelizable? |
|-------|---------------|------------|-----------------|
| Capture + Undistort | 2 | 1.9% | No (hardware) |
| Enhancement | 2 | 1.9% | No (sequential) |
| YOLO Inference | 40 | 37.4% | Partially (CUDA streams) |
| Depth Inference | 27 | 25.2% | Partially (CUDA streams) |
| Depth Fusion | 2 | 1.9% | No (CPU) |
| Tracking | 2 | 1.9% | No (CPU) |
| Decision Engine | 3 | 2.8% | No (CPU) |
| Audio Routing | 2 | 1.9% | No (CPU) |
| **Total Pipeline** | **107** | **100%** | |
| TTS Synthesis | 43 | (Additional) | Yes (separate thread) |
| **End-to-End** | **150** | | |

### Frame Skip Impact

```
Without frame skip (every frame):
  YOLO: 60 fps * 40ms = 2400ms/sec = OVERLOAD
  Depth: 60 fps * 27ms = 1620ms/sec = OVERLOAD
  Total: 4020ms/sec â†’ 0.25 FPS âŒ

With frame skip (YOLO every 3rd, Depth every 12th):
  YOLO: 20 fps * 40ms = 800ms/sec
  Depth: 5 fps * 27ms = 135ms/sec
  Other: 60 fps * 20ms = 1200ms/sec
  Total: 2135ms/sec â†’ 0.47 FPS (still tight)

With frame skip + pipeline optimization:
  Actual measured: 18-22 FPS âœ“
  (Parallel GPU ops + efficient CPU code)
```

### CUDA Streams Benefit (Phase 6)

```
Sequential (Baseline):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO 40ms â”‚â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ 67ms total
â”‚   Depth 27msâ”‚â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Parallel (Phase 6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO 40ms â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Depth 27msâ”‚ (overlapped)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: max(40, 27) + sync = 42ms

Savings: 67 - 42 = 25ms (37% reduction)
```

**Actual gain:** Only ~3ms improvement
**Why?** TensorRT (YOLO) and ONNX (Depth) use different GPU contexts, limiting overlap

---

## Memory Flow

### GPU Memory (VRAM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        NVIDIA RTX 2060 (6GB)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  YOLO TensorRT   â”‚  800 MB         â”‚
â”‚  â”‚  â€¢ Weights       â”‚                 â”‚
â”‚  â”‚  â€¢ Activations   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Depth ONNX      â”‚  500 MB         â”‚
â”‚  â”‚  â€¢ Weights       â”‚                 â”‚
â”‚  â”‚  â€¢ Activations   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  CUDA Context    â”‚  200 MB         â”‚
â”‚  â”‚  â€¢ Kernels       â”‚                 â”‚
â”‚  â”‚  â€¢ Buffers       â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Free Memory     â”‚  4.5 GB (75%)   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CPU Memory (RAM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           System RAM (32GB)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Frame Buffers   â”‚  ~200 MB        â”‚
â”‚  â”‚  â€¢ RGB queue     â”‚  (10 frames)    â”‚
â”‚  â”‚  â€¢ SLAM queue    â”‚  (10 frames)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Detection Lists â”‚  ~50 MB         â”‚
â”‚  â”‚  â€¢ Current frame â”‚                 â”‚
â”‚  â”‚  â€¢ History (100) â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Telemetry Logs  â”‚  ~100 MB        â”‚
â”‚  â”‚  â€¢ JSONL buffers â”‚                 â”‚
â”‚  â”‚  â€¢ Queue         â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Free Memory     â”‚  ~30 GB         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Edge Cases & Error Handling

### 1. No Detections in Frame

```python
# Frame N: Empty street
detections = yolo.detect(frame)  # Returns []

# Pipeline continues
tracked = tracker.update([])  # No tracks to update
decision_engine.process([])   # No commands generated

# Result: Silence (no audio spam)
```

### 2. CUDA Out of Memory

```python
try:
    depth_map = depth_estimator.estimate(frame)
except RuntimeError as e:
    if "out of memory" in str(e):
        logger.warning("CUDA OOM, skipping depth for this frame")
        depth_map = None  # Graceful degradation
        # Continue without depth
```

### 3. Audio Queue Overflow

```python
# AudioRouter: Queue maxsize = 10
try:
    self.audio_queue.put(command, timeout=0.1)
except queue.Full:
    logger.warning(f"Audio queue full, dropping: {command}")
    # Drop oldest command, not current
```

### 4. Observer Frame Drop

```python
# Observer: Frame N+1 arrives before N is processed
rgb_frame = observer.get_rgb_frame()
if rgb_frame is None:
    logger.debug("Frame drop detected")
    continue  # Skip this iteration, wait for next

# Result: Graceful handling, no crash
```

---

## Debugging Tools

### 1. Timing Instrumentation

```python
# Example: Measure YOLO latency
from utils.profiler import Profiler

with Profiler("yolo_inference"):
    detections = yolo.detect(frame)

# Logs: [PROFILER] yolo_inference: 42.3ms
```

### 2. Frame-Level Telemetry

```python
# Telemetry output (performance.jsonl):
{
  "timestamp": 1700000000.123,
  "frame_number": 180,
  "fps": 19.2,
  "yolo_latency_ms": 40.1,
  "depth_latency_ms": 27.3,
  "total_latency_ms": 78.5
}
```

### 3. Detection Logging

```python
# Telemetry output (detections.jsonl):
{
  "timestamp": 1700000000.123,
  "frame_number": 180,
  "detections": [
    {
      "class_name": "person",
      "confidence": 0.89,
      "bbox": [120, 200, 250, 450],
      "distance": 3.2,
      "distance_bucket": "medium",
      "zone": "center",
      "priority": 140
    }
  ]
}
```

---

## Conclusion

This document traced a single frame through the entire Aria Navigation System, from capture to audio output. Key takeaways:

1. **Pipeline Latency:** 107ms (capture â†’ decision)
2. **Bottlenecks:** YOLO (40ms) + Depth (27ms) dominate
3. **Frame Skip:** Essential for real-time performance
4. **Optimization:** TensorRT + ONNX provide 3-4x speedup
5. **Future:** Further gains possible with model quantization (INT8)

**Total Processing Time per Frame:** ~150ms (including TTS)
**Effective FPS:** 18-22 FPS (limited by inference, not I/O)
**User Experience:** Responsive, real-time navigation assistance

---

**Last Updated:** November 25, 2025
**Author:** Roberto Rojas Sahuquillo
